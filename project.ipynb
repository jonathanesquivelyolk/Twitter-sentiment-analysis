{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analytics on the Sem-Eval Dataset Using a DCNN\n",
    "#### Author: Jonathan Esquivel\n",
    ">* Note: We will denote areas with params with a \" * \" in the heading\n",
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils as util\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params:\n",
    "> To change the model simply change these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Process params\n",
    "percent_training = .80 #percent of data used for training\n",
    "\n",
    "val_percent = .1 # percent of whole dataset that is validation (taken from training split)\n",
    "numAugs=1 #number of sugmentations on that data\n",
    "probKeepPositive=1 #Percentage of prosive samples that are trimmed\n",
    "combClasses = True #Combine objective, neutral, and objective-OR-neutral clases\n",
    "maxTokens = 28 #Numebr of tokens used in a tweet (NOTE WE DID NOT TEST ADJUSTING THIS NUMBER. CHANGING MAY CAUSE ERRORS)\n",
    "probKeepNeutral=1 #Percentage of neutral samples that are trimmed (used if comb classes is true)\n",
    "\n",
    "#Training params\n",
    "numEpochs = 50\n",
    "batch_size = 256\n",
    "drop_rate = 0.65\n",
    "cost = 0\n",
    "eps = 1\n",
    "lrate = 0.001 #Learning rate\n",
    "\n",
    "#REstore path for more training (seen in bottom used due to lack of memory on maachine)\n",
    "\n",
    "#qSave = True\n",
    "#savePath = \"./tmp/\" + \"model_v_XXX\" \n",
    "#restore = False #Load a old model\n",
    "#restore_path = \"./tmp/\" + \"model_v_XXX\" #load at this path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Load data:\n",
    ">* Note: We make certain that our data sets have some of each class. <br>\n",
    ">* Note: We are loading our augmented dataset\n",
    "\n",
    "> Param percent_training: percent of the corpus will be training and validation <br>\n",
    "> Param emebdding dims: default is 25 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAM\n",
    "#data found in \"Data/proc_SemEvalClean\"\n",
    "e_Dims=25\n",
    "dataSet = util.semEvalData(\"Data/proc_SemEvalClean.txt\",n_dims=25)\n",
    "qVal = True #Always use validation\n",
    "#validation unused:\n",
    "train_df, test_df, val_df= dataSet.grab_data(percent_training, validation=qVal,percentValidation=val_percent)\n",
    "if combClasses:\n",
    "    train_df = dataSet.combClasses(train_df)\n",
    "    test_df = dataSet.combClasses(test_df)\n",
    "    val_df = dataSet.combClasses(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEFORE AUGMENTATION\")\n",
    "total = len(train_df)  + len(test_df)  + len(val_df)\n",
    "print(\"Data total:\",total)\n",
    "print(\"Count Train:\",len(train_df),\" Percent:\",round(len(train_df)/total * 100,4))\n",
    "print(\"Count Validation:\",len(val_df),\" Percent:\",round(len(val_df)/total *100,4))\n",
    "print(\"Count Test:\",len(test_df),\" Percent:\",round(len(test_df)/total * 100,4))\n",
    "print(\"Train Example: (ndx will be truncated later)\")\n",
    "train_df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment training data:\n",
    "> We do this to increase our datasize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rep,train_df = dataSet.augmentData(train_df,numAugs=numAugs,probKeepPositive=probKeepPositive)\n",
    "print(\"AFTER AUGMENTATION\")\n",
    "print(\"Count Train:\",len(train_df))\n",
    "numDatapoints = len(train_df)\n",
    "print(\"Training:\\n\",train_df[\"Sentiment\"].value_counts(),\"\\n\")\n",
    "print(\"Testing:\\n\",test_df[\"Sentiment\"].value_counts(),\"\\n\")\n",
    "print(\"Validation:\\n\",val_df[\"Sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings:\n",
    ">We note that we will ONLY load the embeddings for words in our corpus. Since the vocab size of the embeddings is MUCH larger than the vocab size of our corpus we do this to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict,totalCount,missingCount= dataSet.load_embeddings(\"Data\\GloVe_Embeddings\\glove.twitter.27B.25d.txt\")\n",
    "\n",
    "#our emebdings / total embeddings in files (get an idea of memory saved)\n",
    "print(\"Corpus vocabulary size:\",len(embeddings_dict.keys()))\n",
    "\n",
    "print(\"SemEval vocabulary Size:\",totalCount)\n",
    "\n",
    "#corpus specific\n",
    "print(\"Vocab in our corpus missing in SemEval:\",missingCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Tokenize data:\n",
    "> Here we will:\n",
    ">1. Tokenize the tweet \n",
    ">2. Replace word tokens with their ids relative to our vocab in the previoud step\n",
    ">3. Pad or truncate to a max tokensize of 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_trainR = train_df[\"Tweet\"]\n",
    "y_train = pd.get_dummies( train_df[\"Sentiment\"]).values\n",
    "\n",
    "x_testR = test_df[\"Tweet\"]\n",
    "y_test = pd.get_dummies(test_df[\"Sentiment\"]).values\n",
    "\n",
    "\n",
    "x_valR = val_df[\"Tweet\"]\n",
    "y_val = pd.get_dummies(val_df[\"Sentiment\"]).values\n",
    "\n",
    "x_train = dataSet.tokenize_data(x_trainR,padLength=maxTokens)\n",
    "x_test = dataSet.tokenize_data(x_testR,padLength=maxTokens)\n",
    "x_val = dataSet.tokenize_data(x_valR,padLength=maxTokens)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X:\",[None] + list(x_train.shape[1:]))\n",
    "print(\"Shape of Y:\",[None] + list(y_train.shape[1:]))\n",
    "numClasses = [None] + list(y_train.shape[1:])\n",
    "arr = np.array([embeddings_dict[word_id] for word_id in x_train[0]])\n",
    "print(\"Shape of X_in:\",[None] + list(arr.shape[:]))\n",
    "print(\"Emebeddings Shape:\",[len(embeddings_dict.keys()),e_Dims])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREAK NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into tf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embVals = np.array([value for wordId,value in embeddings_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClasses = [None] + list(y_train.shape[1:])\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "#x_test = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "#x_val = tf.data.Dataset.from_tensor_slices(x_val)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "#y_test = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "#y_val = tf.data.Dataset.from_tensor_slices(y_val)\n",
    "\n",
    "train_data = tf.data.Dataset.zip((x_train,y_train))\n",
    "#test_data = tf.data.Dataset.zip((x_test,y_test))\n",
    "#val_data = tf.Dataset.zip((x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete unneeded variables\n",
    "del(x_train)\n",
    "del(y_train)\n",
    "del(embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Setup Model:\n",
    "\n",
    "### Placeholders:\n",
    ">X_ids: input sentence tensor:\n",
    ">* 1st-D: This will be None. (Batch size) \n",
    ">* 2nd-D: This will have width equal to the number of maxTokens (including the paddings) \n",
    ">* TYPE: This will have type int32 (since it is a tensor of id's)\n",
    ">* x_train.shape -> [None, 28]\n",
    "\n",
    ">X: input sentence emebedding tensor:\n",
    ">* 1st-D: This will be None. (Batch size) \n",
    ">* 2nd-D: This will have width equal to the embedding dimension size\n",
    ">* 3rd-D: This will have heigth equal to the number of maxTokens (including the paddings)\n",
    ">* TYPE: This will have type float32 (since it is the emebeddings of id's)\n",
    ">* Example:\n",
    ">* X_in.shape -> [None, 28, 25]\n",
    "\n",
    ">Y: output tensor:\n",
    ">* 1st-D: This will be None. (Batch size) \n",
    ">* 2nd-D: This will have the number of possbile classes\n",
    ">* TYPE: this will have float32\n",
    ">* Example: y_train.shape -> [None, 5]\n",
    "\n",
    ">embeddings: the embedding lookup table:\n",
    ">* 1st-D: This will be number of words in our lookup table\n",
    ">* 2nd-D: This will be the size of the embeddings\n",
    ">* Example: embeddings.shape -> [15303, 25]\n",
    "\n",
    ">phase: a boolean for batch normalization\n",
    "\n",
    ">* LearningRate: the learning rate for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ids = tf.placeholder(tf.int32,[None,maxTokens],name=\"X_Ids\")\n",
    "\n",
    "#?\n",
    "#X = tf.placeholder(tf.float32,[None,maxTokens,e_Dims],name=\"X_input\")\n",
    "\n",
    "Y = tf.placeholder(tf.float32,numClasses,name=\"Output_Class\")\n",
    "\n",
    "embeddings = tf.placeholder(tf.float32,[len(embVals),e_Dims],name=\"Embeddings\")\n",
    "\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "learningRate = tf.placeholder(tf.float32, name='Learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2id Lookup:\n",
    "> Convert2Id: This operation will convert our batches of X to X_ids which is what will be looked up <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2id,_ = dataSet.getMapping()\n",
    "#mapping_strings = np.array([value for wordId,value in word2id.items()])\n",
    "\n",
    "\n",
    "#This is a table of strings that map to their ids <CUT1> if we want we can just use this\n",
    "#Xin2Id_table = tf.contrib.lookup.index_table_from_tensor(mapping=mapping_strings,default_value=0)\n",
    "\n",
    "#emblookup\n",
    "X = tf.nn.embedding_lookup(embeddings,X_ids,name=\"Lookup\")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:\n",
    "> Our Model Will consist of 4 parrallel convolutional layers followed by 4 pooling layers <br>\n",
    ">* Note: our convoultional layers will have width equal input width so they will always output [?,1]\n",
    ">* Currently we will assume a maxTokens=28,eDims=25\n",
    "\n",
    "### Convolution 1A:\n",
    ">* Input: [batch_size,maxTokens,embeddingDims]\n",
    ">* Filter: [3,embeddingDims]\n",
    ">* Stride: [1,1] \n",
    ">* Pad: VALID\n",
    ">* Feature Maps: 7\n",
    ">* Output: [26,1,7]\n",
    ">* Activation function: Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1A = tf.Variable(tf.truncated_normal([3, e_Dims, 7], stddev=0.1),name=\"CONV1A_Weights\")\n",
    "b_conv1A = tf.Variable(tf.constant(0.1, shape=[7]),name=\"CONV1A_Bias\") # need 7 biases for 7 outputs\n",
    "conv1A= tf.nn.conv1d(X, W_conv1A, stride=1, padding='VALID',name=\"CONV1A\") + b_conv1A\n",
    "\n",
    "h_conv1A = tf.nn.relu(conv1A,name=\"reluA\")\n",
    "h_conv1A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution 1B:\n",
    ">* Input: [batch_size,maxTokens,embeddingDims] \n",
    ">* Filter: [5,embeddingDims]\n",
    ">* Stride: [1,1] \n",
    ">* Pad: VALID\n",
    ">* Feature Maps: 7\n",
    ">* Output: [24,1,7]\n",
    ">* Activation function: Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1B = tf.Variable(tf.truncated_normal([5, e_Dims, 7], stddev=0.1),name=\"CONV1B_Weights\")\n",
    "b_conv1B = tf.Variable(tf.constant(0.1, shape=[7]),name=\"CONV1B_Bias\") # need 7 biases for 7 outputs\n",
    "conv1B = tf.nn.conv1d(X, W_conv1B, stride=1, padding='VALID',name=\"CONV1B\") + b_conv1B\n",
    "\n",
    "h_conv1B = tf.nn.relu(conv1B,name=\"reluB\")\n",
    "h_conv1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution 1C:\n",
    ">* Input: [batch_size,maxTokens,embeddingDims]\n",
    ">* Filter: [7,embeddingDims]\n",
    ">* Stride: [1,1] \n",
    ">* Pad: VALID\n",
    ">* Feature Maps: 7\n",
    ">* Output: [22,1,7]\n",
    ">* Activation function: Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1C = tf.Variable(tf.truncated_normal([7, e_Dims, 7], stddev=0.1),name=\"CONV1C_Weights\")\n",
    "b_conv1C = tf.Variable(tf.constant(0.1, shape=[7]),name=\"CONV1C_Bias\") # need 7 biases for 7 outputs\n",
    "conv1C = tf.nn.conv1d(X, W_conv1C, stride=1, padding='VALID',name=\"CONV1C\") + b_conv1C\n",
    "\n",
    "h_conv1C = tf.nn.relu(conv1C,name=\"reluC\")\n",
    "h_conv1C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution 1D:\n",
    ">* Input: [batch_size,maxTokens,embeddingDims]\n",
    ">* Filter: [3,embeddingDims]\n",
    ">* Stride: [2,1] \n",
    ">* Pad: VALID\n",
    ">* Feature Maps: 7\n",
    ">* Output: [13,1,7]\n",
    ">* Activation function: Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1D = tf.Variable(tf.truncated_normal([2, e_Dims, 7], stddev=0.1),name=\"CONV1D_Weights\")\n",
    "b_conv1D = tf.Variable(tf.constant(0.1, shape=[7]),name=\"CONV1D_Bias\") # need 7 biases for 7 outputs\n",
    "conv1D = tf.nn.conv1d(X, W_conv1D, stride=1, padding='VALID',name=\"CONV1D\") + b_conv1D\n",
    "\n",
    "h_conv1D = tf.nn.relu(conv1D,\"reluD\")\n",
    "h_conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool 1A:\n",
    ">* Input: [batch_size,26,7]\n",
    ">* Ksize: 2\n",
    ">* Stride: 2\n",
    ">* Pad: VALID\n",
    ">* Output: [batch_size,12,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool1A = tf.layers.max_pooling1d(h_conv1A,pool_size=2,strides=1, padding='VALID',name=\"1A_POOL\") #max_pool on 26 length of 7 channels\n",
    "pool1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool 1B:\n",
    ">* Input: [batch_size,24,7]\n",
    ">* Ksize: 2\n",
    ">* Stride: 2\n",
    ">* Pad: VALID\n",
    ">* Output: [batch_size,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool1B = tf.layers.max_pooling1d(h_conv1B,pool_size=2,strides=1, padding='VALID',name=\"1B_POOL\") #max_pool on 24 length of 7 channels\n",
    "pool1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool 1C:\n",
    ">* Input: [batch_size,12,7]\n",
    ">* Ksize: 2\n",
    ">* Stride: 1\n",
    ">* Pad: VALID\n",
    ">* Output: [batch_size,10,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool1C = tf.layers.max_pooling1d(h_conv1C,pool_size=2,strides=1, padding='VALID',name=\"1C_POOL\") #max_pool on 24 length of 7 channels\n",
    "pool1C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool 1D:\n",
    ">* Input: [batch_size,13,7]\n",
    ">* Ksize: 2\n",
    ">* Stride: 1\n",
    ">* Pad: VALID\n",
    ">* Output: [batch_size,11,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool1D = tf.layers.max_pooling1d(h_conv1D,2,strides=1, padding='VALID',name=\"1D_POOL\") #max_pool on 24 length of 7 channels\n",
    "pool1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat parallel layers to a Matrix:\n",
    ">* Concat Input(s): \n",
    ">* pool1A: [25]*7\n",
    ">* pool1B: [23]*7\n",
    ">* pool1C: [21]*7\n",
    ">* pool1D: [12]*7\n",
    ">* Output: [81,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_layer = tf.concat([pool1A,pool1B,pool1C,pool1D],1,name=\"Concat_Layers\")\n",
    "#cat_layer = tf.concat([pool1A,pool1B,pool1C],1,name=\"Concat_Layers\")\n",
    "cat_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer = tf.reshape(cat_layer, [-1, 95*7])\n",
    "fc_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fc_layer = tf.contrib.layers.batch_norm(fc_layer, center=True, scale=True, is_training=phase,scope='bn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesnt seem to learn much here...\n",
    "\n",
    "#W_fc1 = tf.Variable(tf.truncated_normal([567, 128], stddev=0.1),name=\"FULL1_Weights\") #273 neurons\n",
    "#b_fc1 = tf.Variable(tf.constant(0.1, shape=[128]),name=\"FULL1_Bias\") # 5 possibilities\n",
    "\n",
    "#fc1=tf.matmul(norm_fc_layer, W_fc1) + b_fc1\n",
    "#fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer_drop = tf.nn.dropout(norm_fc_layer, keep_prob,name=\"Dropout\")\n",
    "layer_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = tf.Variable(tf.truncated_normal([665, numClasses[1]], stddev=0.1),name=\"FULL2_Weights\") #273 neurons\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[numClasses[1]]),name=\"FULL2_Bias\") # 5 possibilities\n",
    "\n",
    "fc2=tf.matmul(layer_drop, W_fc2) + b_fc2\n",
    "fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_CNN= tf.nn.softmax(fc2)\n",
    "y_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss:\n",
    ">We will use l2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost_OP = tf.nn.l2_loss(y_CNN-Y, name=\"squared_error_cost\")\n",
    "\n",
    "##cost_OP = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,logits=fc2,name=\"cross_entropy\")#WRONG?\n",
    "\n",
    "#cost_OP = -tf.reduce_sum(Y * tf.log(y_CNN + 1e-5)) #Not great....\n",
    "cost_OP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAM:\n",
    "#learningRate = 0.001\n",
    "\n",
    "#adamoptimizer\n",
    "\n",
    "#learningRate = tf.train.exponential_decay(learning_rate=0.008,\n",
    "#                                         global_step= 1,\n",
    "#                                         decay_steps=numDatapoints, #this is the number of datapoints...\n",
    "#                                         decay_rate= 0.95,\n",
    "#                                         staircase=True)\n",
    "\n",
    "#Defining our Gradient Descent\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "        training_OP = tf.train.AdamOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.initializers.global_variables()\n",
    "\n",
    "# argmax(activation_OP, 1) returns the label with the most probability\n",
    "# argmax(Y, 1) is the correct label\n",
    "correct_predictions_OP = tf.equal(tf.argmax(y_CNN,1),tf.argmax(Y,1))\n",
    "\n",
    "# If every false prediction is 0 and every true prediction is 1, the average returns us the accuracy\n",
    "accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "\n",
    "# Summary op for regression output\n",
    "activation_summary_OP = tf.summary.histogram(\"output\", y_CNN)\n",
    "\n",
    "# Summary op for accuracy\n",
    "accuracy_summary_OP = tf.summary.scalar(\"accuracy\", accuracy_OP)\n",
    "\n",
    "# Summary op for cost\n",
    "cost_summary_OP = tf.summary.scalar(\"cost\", cost_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Running the Model:\n",
    "\n",
    "### Inputs: MODIFY THIS TO SAVE OR LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from above Change if desired on loading a model\n",
    "#Training params\n",
    "'''\n",
    "numEpochs = 50\n",
    "batch_size = 256\n",
    "drop_rate = 0.66\n",
    "cost = 0\n",
    "eps = 1\n",
    "lrate = 0.001 #Learning rate\n",
    "'''\n",
    "\n",
    "#Before re-running be sure to rest this should you want to train again\n",
    "qSave = False\n",
    "savePath = \"./tmp/\" + \"model_v_XXXXXXXXX\" \n",
    "restore = False #Load a old model\n",
    "restore_path = \"./tmp/\" + \"model_v_XXXXXXX\" #load at this path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    if not restore:\n",
    "        sess.run(init_op)\n",
    "    else:\n",
    "        saver.restore(sess, restore_path)\n",
    "    \n",
    "    ##Sumary steps:#######################################################################################################\n",
    "    # Summary ops to check how variables (W, b) are updating after each iteration\n",
    "    CONV1A_weightSummary = tf.summary.histogram(\"CONV1A_weights\", W_conv1A)\n",
    "    CONV1A_biasSummary = tf.summary.histogram(\"CONV1A_biases\", b_conv1A)\n",
    "    \n",
    "    CONV1B_weightSummary = tf.summary.histogram(\"CONV1B_weights\", W_conv1B)\n",
    "    CONV1B_biasSummary = tf.summary.histogram(\"CONV1B_biases\", b_conv1B)\n",
    "    \n",
    "    CONV1C_weightSummary = tf.summary.histogram(\"CONV1C_weights\", W_conv1C)\n",
    "    CONV1C_biasSummary = tf.summary.histogram(\"CONV1C_biases\", b_conv1C)\n",
    "    \n",
    "    CONV1D_weightSummary = tf.summary.histogram(\"CONV1D_weights\", W_conv1D)\n",
    "    CONV1D_biasSummary = tf.summary.histogram(\"CONV1D_biases\", b_conv1D)\n",
    "    \n",
    "    #FULL1_weightSummary = tf.summary.histogram(\"FULL1_weights\", W_fc1.eval())\n",
    "    #FULL1_biasSummary = tf.summary.histogram(\"FULL1_biases\", b_fc1.eval())\n",
    "\n",
    "    \n",
    "    FULL2_weightSummary = tf.summary.histogram(\"FULL2_weights\", W_fc2)\n",
    "    FULL2_biasSummary = tf.summary.histogram(\"FULL2_biases\", b_fc2)\n",
    "    # Merge all summaries\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Summary writer \n",
    "    writer = tf.summary.FileWriter(\"Logged_Summaries2\", sess.graph)\n",
    "    \n",
    "    \n",
    "    ######################################################################################################################\n",
    "    \n",
    "\n",
    "    s = 0\n",
    "    for epoch in range(1,numEpochs+1):\n",
    "        batch_cnt = 0\n",
    "        #Batch data:##########################################################################################################\n",
    "        train_data = train_data.shuffle(buffer_size=10000)\n",
    "        batched_dataset = train_data.batch(batch_size)\n",
    "        train_Iterator = batched_dataset.make_one_shot_iterator()\n",
    "        next_training_batch = train_Iterator.get_next()\n",
    "       \n",
    "        \n",
    "        ######################################################################################################################\n",
    "        #loop through batches\n",
    "        while True:\n",
    "            s+=1\n",
    "            try:\n",
    "                #Split batches\n",
    "                train_batch = sess.run(next_training_batch)\n",
    "                x_batch, y_batch = train_batch[0], train_batch[1]\n",
    "                \n",
    "                #Batch training:\n",
    "                summ,step = sess.run([merged,training_OP], \n",
    "                                     feed_dict={X_ids:x_batch, Y:y_batch, \n",
    "                                                embeddings:embVals,\n",
    "                                                keep_prob:drop_rate,\n",
    "                                                phase:True,\n",
    "                                               learningRate:lrate})\n",
    "                \n",
    "                writer.add_summary(summ, global_step=s)\n",
    "                \n",
    "                #Write steps:\n",
    "                if batch_cnt % 15 == 0:\n",
    "                    # Add epoch to epoch_values\n",
    "                    #epoch_values.append(batch_cnt)\n",
    "                    # Generate accuracy stats on batch data\n",
    "                    train_accuracy, newCost = sess.run([accuracy_OP, cost_OP], \n",
    "                                                       feed_dict={X_ids: x_batch, Y: y_batch, \n",
    "                                                                  embeddings:embVals,\n",
    "                                                                  keep_prob:1,\n",
    "                                                                  phase:False,\n",
    "                                                                 learningRate:lrate})\n",
    "                    # Add accuracy to live graphing variable\n",
    "                    #accuracy_values.append(train_accuracy)\n",
    "                    # Add cost to live graphing variable\n",
    "                    #cost_values.append(newCost)\n",
    "                    # Re-assign values for variables\n",
    "                    eps = abs(newCost - cost)\n",
    "                    cost = newCost              \n",
    "                    \n",
    "                    \n",
    "                    #generate print statements\n",
    "                    if batch_cnt % 45:\n",
    "                        print(\"batch %d, training accuracy %g, cost %g, change in cost %g, lrate %g\"\n",
    "                              %(batch_cnt, train_accuracy, newCost, eps,lrate))\n",
    "                    \n",
    "                batch_cnt+=1\n",
    "                \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Finished epoch:\",epoch)\n",
    "                break\n",
    "            \n",
    "        #End of epoch handle:\n",
    "        #Evaluate validation data:\n",
    "        val_acc = sess.run(accuracy_OP,feed_dict={X_ids: x_val, \n",
    "                                              Y: y_val, \n",
    "                                              embeddings:embVals,\n",
    "                                              keep_prob:1,\n",
    "                                                phase:False,\n",
    "                                                 learningRate:lrate})\n",
    "        \n",
    "        print(\"Accuracy on Validation:\",val_acc,\"on completion of epoch\",epoch)\n",
    "        \n",
    "    #Evaluate test set\n",
    "    test_acc = sess.run(accuracy_OP,feed_dict={X_ids: x_test, \n",
    "                                              Y: y_test, \n",
    "                                              embeddings:embVals,\n",
    "                                              keep_prob:1,\n",
    "                                            phase:False,\n",
    "                                              learningRate:lrate})\n",
    "\n",
    "    print(\"Final accuracy on Test Set:\",test_acc,\"on completion of epoch\",epoch)\n",
    "    \n",
    "    \n",
    "    if qSave:\n",
    "        save_path = saver.save(sess, savePath)\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Predict Tweets on the fly\n",
    "#def predictTweet(tweet):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
